# Comment on CH 3

## Overview of CH 3 and data ethics

This chapter is quite different to the previous: it's largely a philosophical discussion, without coding, and has been co-authored with [Dr Rachel Thomas](https://en.wikipedia.org/wiki/Rachel_Thomas_(academic))

However it is an extremely important chapter since it deals with the subject of *data ethics* - a topic that will likely become increasingly important as Deep Learning is implemented across more areas of life.

To keep this blog post brief, I will getting into a discussion on 'data ethics' meaning. Instead I will just reiterate the definition proposed by Howard, Gugger and Thomas (which itself is based on a [definition](https://www.scu.edu/ethics/ethics-resources/ethical-decision-making/what-is-ethics/) from the Markkula Center for Applied Ethics) :

>"Ethics is based on well-founded standards of right and wrong that prescribe what humans ought to do, usually in terms of rights, obligations, benefits to society, fairness, or specific virtues.

>Ethics refers to the study and development of one's ethical standards."

With this blog post I intend to summarise what I feel are the main points Howard, Gugger and Thomas make and offer my own comments.

*[Link to video lecture](https://youtu.be/krIVOb23EH8)*

## Ethical and practical implications for using Deep Learning

Many of the ethical issues that emerge from integrating Deep Learning into tasks steam from its advantages. So we might start by asking why would a company or government choose to use Deep Learning in the first place?

Well it offers potentially more systematic, accurate and accountable ways to process data and make predictions versus employing a large team of human workers. I say 'potentially' as properly implementing Deep Learning is extremely difficult.

From my reading of this chapter, there appear to be two fundamental cause of problems. The first is where a project has been *poorly conceived* before even starting. Examples of this cited by Howard, Gugger and Thomas include [IBM working with the Nazi government in Germany](https://en.wikipedia.org/wiki/IBM_and_the_Holocaust) or more recently technicians at [VW manipulating](https://en.wikipedia.org/wiki/Volkswagen_emissions_scandal) tests so their engines appeared to be more environmentally friendly.

The second issue is more *operational*. Even if a great deal of care has been put into a evaluating project's merits beforehand, and it is indeed a good idea, if not properly implemented on an ongoing basis unexpected and detrimental outcomes can occur.

Examples of this situation include using Machine Learning to make [assessments for disability claims](https://www.theverge.com/2018/3/21/17144260/healthcare-medicaid-algorithm-arkansas-cerebral-palsy) more accurate, less bias and consistent but instead resulting in people with genuine disabilities being denied care, or receive less cared, due to inaccurate data and buggy software.

Perhaps even more maliciously are situations where companies are fully aware of ethical problems in their applications but ignoring them due to profit making incentives. An example, which I'm sure everyone will be familiar with, is YouTube pushing users towards more extreme content over time (conspiracy theories for instance) in order encourage people to stay on their service and make more advertising revenue.

## How to approach ethical issues in Deep Learning

How should we begin to address these ethical problems? I believe there are three distinct areas to consider: personal, organisational and governmental. These approaches can be summarised as follows

Area | Responsibilities
---- | ----------------
Personal | Consider ethical dimensions of your projects/assigned work
Organisational | Does your workplace have a sufficiently robust internal code of conduct, do employees observe it? <br> Are teams sufficiently diverse to avoid group think and the consequential bad behavior
Governmental | Is it necessary to intervene to private private profitability at the expense of overall poor social outcomes
