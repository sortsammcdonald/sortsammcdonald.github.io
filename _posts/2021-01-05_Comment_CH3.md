# Comment on CH 3

## Overview of CH 3 and data ethics

This chapter is quite different to the previous: it's largely a philosophical discussion, without coding, and has been co-authored with [Dr Rachel Thomas](https://en.wikipedia.org/wiki/Rachel_Thomas_(academic))

However it is an extremely important chapter since it deals with the subject of *data ethics* - a topic that will likely become increasingly important as Deep Learning is implemented across more areas of life.

To keep this blog post brief, I will getting into a discussion on 'data ethics' meaning. Instead I will just reiterate the definition proposed by Howard, Gugger and Thomas (which itself is based on a [definition](https://www.scu.edu/ethics/ethics-resources/ethical-decision-making/what-is-ethics/) from the Markkula Center for Applied Ethics) :

>"Ethics is based on well-founded standards of right and wrong that prescribe what humans ought to do, usually in terms of rights, obligations, benefits to society, fairness, or specific virtues.

>Ethics refers to the study and development of one's ethical standards."

With this blog post I intend to summarise what I feel are the main points Howard, Gugger and Thomas make and offer my own comments.

## Ethical and practical implications for using Deep Learning

Many of the ethical issues that emerge from integrating Deep Learning into tasks steam from its advantages. So we might start by asking why would a company or government choose to use Deep Learning in the first place?

Well it offers potentially more systematic, accurate and accountable ways to process data and make predictions versus employing a large team of human workers. I say 'potentially' as properly implementing Deep Learning is extremely difficult.

From my reading of this chapter, there appear to be two fundamental cause of problems. The first is where a project has been *poorly conceived* before even starting. Examples of this cited by Howard, Gugger and Thomas include [IBM working with the Nazi government in Germany](https://en.wikipedia.org/wiki/IBM_and_the_Holocaust) or more recently technicians at [VW manipulating](https://en.wikipedia.org/wiki/Volkswagen_emissions_scandal) tests so their engines appeared to be more environmentally friendly.

The second issue is more *operational*. Even if a great deal of care has been put into a evaluating project's merits beforehand, and it is indeed a good idea, if not properly implemented on an ongoing basis unexpected and detrimental outcomes can occur.

Examples of this situation include using Machine Learning to make [assessments for disability claims](https://www.theverge.com/2018/3/21/17144260/healthcare-medicaid-algorithm-arkansas-cerebral-palsy) more accurate, less bias and consistent but instead resulting in people with genuine disabilities being denied care, or receive less cared, due to inaccurate data and buggy software.

Perhaps even more maliciously are situations where companies are fully aware of ethical problems in their applications but ignoring them due to profit making incentives. An example, which I'm sure everyone will be familiar with, is YouTube pushing users towards more extreme content over time (conspiracy theories for instance) in order encourage people to stay on their service and make more advertising revenue.

## How to approach ethical issues in Deep Learning

How should we begin to address these ethical problems? I believe there are three distinct areas to consider: personal, organisational and governmental. These approaches can be summarised as follows

Area | Responsibilities
---- | ----------------
Personal | Consider ethical dimensions of your projects/assigned work
Organisational | Does your workplace have a sufficiently robust internal code of conduct, do employees observe it? <br> Are teams sufficiently diverse to avoid group think and the consequential bad behavior
Governmental | Is it necessary to intervene to private private profitability at the expense of overall poor social outcomes

<!--

## Some reflections on these challenges

Debatably, however, these issues emerge from fundamentaly problem in human society, can we really expect machines to do any better? 

In fact, we already see some questionable, if not outright immoral applications of DL today. From seemly trivial examples such as a [former Fed Chairman]() being denied a mortage due to a leader's algorithm having a bias against the self-employed to [suspucions of governments using bots to generate disinformation with the intent of manipulating rival's election results](). Such incidents will only become more common unless society properly evalutes both the technicaly and conceptual application of Deep Learning.

### Poor thought

In respect to poor thought. This frequently stems from individuals or teams not fully considering the ethical implciations of a project before implementing. While I suspect it's impossible to fully anticipate all potential ethical consequences of any course of action - could a young Mark Zuckerberg have anticipated that his burgeoning social network could be the site of state sponsored information propaganda? In fairness though, the moral merits of an app to rate females on Harvards campus does seem rather dubious in the first place.

The issue here is that companies don't necessarily have a good reason to instill good behaviour in their users. There always a delemma here of course. Obviously many firms manufacture substances that aren't necessarily health for us (sugary snacks/drinks, tabacco, alcohol), and although these substances are controlled and regulated by government, there does not seem to be the same burden placed on these firms as with software companies creating similar quasi-addictive time wasting substances.a

-->
